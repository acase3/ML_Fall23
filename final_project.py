# -*- coding: utf-8 -*-
"""FINAL_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eEGdSl8_x3NZ0MEBcn1v752cQuxipXmV
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

MY_PATH = '/content/drive/MyDrive/'



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df_noise = pd.read_csv(MY_PATH+"waveforms_noise.csv")
df_EQ = pd.read_csv(MY_PATH+"waveforms_final.csv")
df_EQ_ex = pd.read_csv(MY_PATH+"waveforms_extra.csv")
print('done!')

df_noise.describe()

df_EQ.describe()

subset = df_EQ_ex.iloc[100:2850]

frames = [df_EQ,subset, df_noise]

df_waves = pd.concat(frames)

from sklearn.model_selection import train_test_split


df_waves = df_waves.drop(df_waves.columns[0], axis=1)
df_waves.rename(columns={"511": "class"}, inplace=True)
df_waves.head()

from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(df_waves, test_size=0.2, random_state=2023)
df_train = df_train.dropna()
df_test = df_test.dropna()
y_train = np.array(df_train['class'])
y_test = np.array(df_test['class'])
df_train = df_train.drop(['class'], axis=1)
df_test = df_test.drop(['class'], axis=1)
print(df_train.shape)
print(df_test.shape)

X_train = np.array(df_train)
X_test = np.array(df_test)

print(y_train[1])

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test  = scaler.transform(X_test)

from sklearn.decomposition import PCA

pca = PCA(n_components=100, random_state=2023)
pca.fit(X_train)
plt.figure(figsize=(12,4))
plt.plot(np.arange(1,101), np.cumsum(pca.explained_variance_ratio_), marker='.')
plt.ylim([0,1])
plt.show()

print(np.round( np.cumsum(pca.explained_variance_ratio_[0:50]), 4))

from sklearn.decomposition import PCA
n_components = 50
pca = PCA(n_components=n_components)
pca.fit(X_train)

Xpca_train = pca.transform(X_train)
Xpca_test = pca.transform(X_test)

import matplotlib.colors as cm
colors = ['black','red']
a = np.empty(len(y_train))
for b in np.arange(0,len(y_train), 1):
  if y_train[b] == 'EQ':
    a[b] = 1
  else:
    a[b] = 4

fig, ax = plt.subplots(1,3, figsize=(18,6))
ax[0].scatter(Xpca_train[:,0], Xpca_train[:,1], c =a,  cmap=cm.ListedColormap(colors), marker='x', s=20, alpha=0.2)
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[1].scatter(Xpca_train[:,2], Xpca_train[:,3], c=a, cmap=cm.ListedColormap(colors), marker='x', s=20, alpha=0.2)
ax[1].set_xlabel('PC3')
ax[1].set_ylabel('PC4')
ax[2].scatter(Xpca_train[:,2], Xpca_train[:,3], c=a, cmap=cm.ListedColormap(colors), marker='x', s=20, alpha=0.2)
ax[2].set_xlabel('PC5')
ax[2].set_ylabel('PC6')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(n_estimators=200, max_features="sqrt", oob_score=True, random_state=2023)
rf.fit(Xpca_train, y_train)

rf_val_score = rf.oob_score_

print('Random forest validation accuracy:', np.round(rf_val_score,4))

y_test_rf = rf.predict(Xpca_test)
print('Random Forest:', np.round(accuracy_score(y_test, y_test_rf),4))

from sklearn.metrics import ConfusionMatrixDisplay

fig, ax = plt.subplots(1,1, figsize=(12,12))
ConfusionMatrixDisplay.from_estimator(rf, Xpca_test, y_test,normalize=None,ax=ax,xticks_rotation='vertical')
plt.title('Random Forest performance on test data')
plt.show()

from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit

fig, ax= plt.subplots(nrows = 1, ncols = 1, figsize=(10, 6), sharey=True)

common_params = {
    "X": Xpca_train,
    "y": y_train,
    "train_sizes": np.linspace(0.1, 1.0, 6),
    "cv": ShuffleSplit(n_splits=10, test_size=0.2, random_state=0),
    "score_type": "both",
    "n_jobs": 5,
    "line_kw": {"marker": "o"},
    "std_display_style": "fill_between",
    "score_name": "Accuracy",
}


LearningCurveDisplay.from_estimator(rf, **common_params, ax=ax)
handles, label = ax[1].get_legend_handles_labels()
ax.legend(handles[:2], ["Training Score", "Test Score"])
ax.set_title(f"Learning Curve for {rf.__class__.__name__}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(n_estimators=200, max_features="sqrt", oob_score=True, random_state=2023)
rf.fit(X_train, y_train)

rf_val_score = rf.oob_score_

print('Random forest validation accuracy:', np.round(rf_val_score,4))

y_test_rf = rf.predict(X_test)
print('Random Forest:', np.round(accuracy_score(y_test, y_test_rf),4))

from sklearn.metrics import ConfusionMatrixDisplay

fig, ax = plt.subplots(1,1, figsize=(12,12))
ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test,normalize=None,ax=ax,xticks_rotation='vertical')
plt.title('Random Forest performance on test data')
plt.show()

##NUERAL NETWORK

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

MY_PATH = '/content/drive/MyDrive/'



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df_noise = pd.read_csv(MY_PATH+"waveforms_noise.csv")
df_EQ = pd.read_csv(MY_PATH+"waveforms_final.csv")
print('done!')

frames = [df_EQ, df_noise]

df_waves = pd.concat(frames)

from sklearn.model_selection import train_test_split


df_waves = df_waves.drop(df_waves.columns[0], axis=1)
df_waves.rename(columns={"511": "class"}, inplace=True)
df_waves.describe()

from sklearn.model_selection import train_test_split

df_train_VAL, df_test = train_test_split(df_waves, test_size=0.2, random_state=2023)
df_train_VAL = df_train_VAL.dropna()
df_test = df_test.dropna()
df_train, df_val= train_test_split(df_train_VAL, test_size=0.25, random_state=1234)

y_train_VAL = np.array(df_train_VAL['class'])
y_test = np.array(df_test['class'])
y_train = np.array(df_train['class'])
y_val = np.array(df_val['class'])
df_train = df_train.drop(['class'], axis=1)
df_test = df_test.drop(['class'], axis=1)
df_val = df_val.drop(['class'], axis=1)
df_train_VAL = df_train_VAL.drop(['class'], axis=1)
print(df_train.shape)
print(df_test.shape)

X_train = np.array(df_train)
X_test = np.array(df_test)
X_train_VAL = np.array(df_train_VAL)
X_val = np.array(df_val)
print(y_train[1])

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense

tf.config.run_functions_eagerly(True)
tf.random.set_seed(123)

from sklearn.preprocessing import OneHotEncoder
onehot = OneHotEncoder()

y_train_onehot    = onehot.fit_transform(y_train.reshape(-1,1)).toarray()
y_trainval_onehot = onehot.fit_transform(y_train_VAL.reshape(-1,1)).toarray()
y_val_onehot      = onehot.fit_transform(y_val.reshape(-1,1)).toarray()
y_test_onehot     = onehot.fit_transform(y_test.reshape(-1,1)).toarray()

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score

nfolds = 5
hidden_nodes = np.arange(3,21,1)
val_scores = np.zeros((hidden_nodes.shape[0], nfolds))

for i, nodes in enumerate(hidden_nodes):
  print(nodes, 'nodes')
  mlp = MLPClassifier(hidden_layer_sizes=[nodes, nodes], activation='relu', max_iter=5000)
  val_scores[i,:] = cross_val_score(mlp, Xpca_train, y_train, cv=nfolds, verbose=1)

plt.figure(figsize=(12,6))
plt.plot(hidden_nodes, np.mean(val_scores, axis=1), marker='o')
plt.plot(hidden_nodes, val_scores, c='black', alpha=0.1)
plt.xlabel('nodes in hidden layers')
plt.ylabel('validation accuracy')
plt.xlim([hidden_nodes[0]-1, hidden_nodes[-1]+1])
plt.xticks(np.arange(hidden_nodes[0]-1, hidden_nodes[-1]+2))
plt.show()

my_nodes = 14 ##change this

my_nodes = 10

mlp = MLPClassifier(hidden_layer_sizes=[my_nodes, my_nodes], activation='relu', max_iter=5000, random_state=2023)
val_score = cross_val_score(mlp, Xpca_train, y_train, cv=nfolds)
mlp_val_score = np.mean(val_score)

mlp.fit(Xpca_train, y_train)

print('hidden nodes:', my_nodes)
print('MLP validation accuracy:', np.round(mlp_val_score, 4))

from sklearn.metrics import accuracy_score

y_test_mlp = mlp.predict(Xpca_test)
print('MLP (sklearn):', np.round(accuracy_score(y_test, y_test_mlp),4))

from sklearn.metrics import ConfusionMatrixDisplay

fig, ax = plt.subplots(1,1, figsize=(12,12))
ConfusionMatrixDisplay.from_estimator(mlp, Xpca_test, y_test,normalize=None,ax=ax,xticks_rotation='vertical')
plt.title('mlp performance on test data')
plt.show()

my_nodes = 14 ##change this
model1 = Sequential()
model1.add(Dense(my_nodes, input_dim=511, activation='relu'))
model1.add(Dense(my_nodes, activation='relu'))
model1.add(Dense(2, activation='softmax'))

model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model1.summary()
print(np.shape(y_val_onehot))
out = model1.fit(X_train, y_train_onehot, validation_data = (X_val, y_val_onehot),  epochs=800, batch_size=200, verbose=1)

fig, ax = plt.subplots(1,2, figsize=(18,6))
ax[0].plot(out.history['accuracy'],linewidth=2.0)
ax[0].plot(out.history['val_accuracy'],linewidth=2.0)
ax[0].set_xlabel('epochs')
ax[0].set_ylabel('accuracy')
ax[0].set_ylim([0.0,1.0])
ax[0].set_xlim([0, 800])
ax[0].legend(['Train', 'Validation'])
ax[1].plot(out.history['loss'],linewidth=2.0)
ax[1].plot(out.history['val_loss'],linewidth=2.0)
ax[1].set_xlabel('epochs')
ax[1].set_ylabel('loss')
ax[1].set_ylim([0, 5])
ax[1].set_xlim([0, 800])
ax[1].legend(['Train', 'Validation'])
plt.show()

my_epochs = 250 ###need to change that
keras_val_score = out.history['val_accuracy'][my_epochs]
print('number of epochs:', my_epochs)
print('ANN validation accuracy:', np.round(keras_val_score,4))

model = Sequential()
model.add(Dense(my_nodes, input_dim=511, activation='relu'))
model.add(Dense(my_nodes, activation='relu'))
model.add(Dense(my_nodes, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train_VAL, y_trainval_onehot, epochs=my_epochs, batch_size=200, verbose=0)

"""logistic regression baseline"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

MY_PATH = '/content/drive/MyDrive/'



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df_noise = pd.read_csv(MY_PATH+"waveforms_noise.csv")
df_EQ = pd.read_csv(MY_PATH+"waveforms_final.csv")
print('done!')

frames = [df_EQ, df_noise]

df_waves = pd.concat(frames)

from sklearn.model_selection import train_test_split


df_waves = df_waves.drop(df_waves.columns[0], axis=1)
df_waves.rename(columns={"511": "class"}, inplace=True)
df_waves.describe()

df_train, df_test = train_test_split(df_waves, test_size=0.2, random_state=2023)
df_train = df_train.dropna()
df_test = df_test.dropna()
y_train = np.array(df_train['class'])
y_test = np.array(df_test['class'])
df_train = df_train.dropna()
df_test = df_test.dropna()
df_train = df_train.drop(['class'], axis=1)
df_test = df_test.drop(['class'], axis=1)
print(df_train.shape)
print(df_test.shape)

X_train = np.array(df_train)
X_test = np.array(df_test)


print(y_train)


from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test  = scaler.transform(X_test)
#pca = PCA(n_components=2000, random_state=2023)
##pca.fit(X_train)
#plt.figure(figsize=(12,4))
#plt.plot(np.arange(1,2001), np.cumsum(pca.explained_variance_ratio_), marker='.')
#plt.ylim([0,1])
#plt.show()

#print(np.round( np.cumsum(pca.explained_variance_ratio_[0:20]), 4))

n_components = 50
pca = PCA(n_components=n_components)
pca.fit(X_train)

Xpca_train = pca.transform(X_train)
Xpca_test = pca.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

nfolds = 10
logit = LogisticRegression(max_iter = 1000,random_state=2023) #had to increase the maxiter to get it to reach convergence
val_score = cross_val_score(logit, Xpca_train, y_train, cv=nfolds)
logit_vals_score = np.mean(val_score)

logit.fit(Xpca_train, y_train)

print('Logistic Regression validation accuracy', np.round(logit_vals_score,4))

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score

fig, ax = plt.subplots(1,1, figsize=(12,12))
ConfusionMatrixDisplay.from_estimator(logit, Xpca_test, y_test,normalize=None,ax=ax,xticks_rotation='vertical')
plt.title('Logistic performance on test data')
plt.show()

Y_logit = logit.predict(Xpca_test)
print('Logistic Regression:', np.round(accuracy_score(y_test, Y_logit),4))

"""gradient boosting random forest xg boost"""

